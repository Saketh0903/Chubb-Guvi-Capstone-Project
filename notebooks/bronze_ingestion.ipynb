{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b18cebc-d0a9-42e1-90f0-344f752e2f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *;\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01aecc0-6b33-405e-85d8-6557dfdf07f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-05\n"
     ]
    }
   ],
   "source": [
    "# Get current date using current_timestamp\n",
    "\n",
    "ingest_date = spark.sql(\"SELECT current_date()\").collect()[0][0]\n",
    "\n",
    "print(ingest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a5f137-a411-4700-b556-c96f4fa1d8b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Companies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe55ffc-f662-486e-898e-94cbc62d2d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/capstone/default/raw_data/companies/incremental/2026-01-05/\n"
     ]
    }
   ],
   "source": [
    "companies_inc_path=f\"/Volumes/capstone/default/raw_data/companies/incremental/{ingest_date}/\"\n",
    "\n",
    "print(companies_inc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866e3d82-0d6a-4790-a02b-7be63b41a905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516b1c1e-9204-4735-ba77-82ad114a5fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import uuid\n",
    "\n",
    "def write_log(table_name, source_system, log_table):\n",
    "    spark.createDataFrame(\n",
    "        [(str(uuid.uuid4()), table_name, source_system)],\n",
    "        [\"log_id\", \"table_name\", \"source_system\"]\n",
    "    ) \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"created_at\", current_timestamp()) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffec7230-22a4-4f1a-803d-41305676b6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No incremental data found at /Volumes/capstone/default/raw_data/companies/incremental/2026-01-05/. Skipping ingestion.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    companies_inc = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(companies_inc_path)\n",
    "    )\n",
    "\n",
    "    companies_bronze = (\n",
    "        companies_inc\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(f\"{companies_inc_path}/companies.csv\"))\n",
    "        .withColumn(\"load_type\", lit(\"incremental\"))\n",
    "    )\n",
    "\n",
    "    companies_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"capstone.default.bronze_companies\")\n",
    "\n",
    "    write_log(\"capstone.default.bronze_companies\", \"default\", \"capstone.default.bronze_log\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"No incremental data found at {companies_inc_path}. Skipping ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beda1000-8ba9-4e66-850d-88c3f8e1d215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Daily prices data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97aee89-e7d3-4b06-b50b-2b5f710e8f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prices_inc_path=f\"/Volumes/capstone/default/raw_data/daily_prices/incremental/{ingest_date}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf22104-5d4a-4ab6-9fe0-8502b5b94b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    daily_prices_inc = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(prices_inc_path)\n",
    "    )\n",
    "\n",
    "    daily_prices_bronze = (\n",
    "        daily_prices_inc\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(f\"{prices_inc_path}/daily_prices.csv\"))\n",
    "        .withColumn(\"load_type\", lit(\"incremental\"))\n",
    "    )\n",
    "\n",
    "    daily_prices_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"capstone.default.bronze_daily_prices\")\n",
    "\n",
    "    write_log(\"capstone.default.bronze_daily_prices\", \"default\", \"capstone.default.bronze_log\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"No incremental data found at {prices_inc_path}. Skipping ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bc34c3-6b80-4997-8903-d6670c4a02e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Traders data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f103a3e6-4be3-4c35-a365-3bf579b5161f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traders_inc_path=f\"/Volumes/capstone/default/raw_data/traders/incremental/{ingest_date}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce533c0-4c9d-474f-921e-26cf2db2724f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No incremental data found at /Volumes/capstone/default/raw_data/traders/incremental/2026-01-05/. Skipping ingestion.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    traders_inc=(\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(traders_inc_path)\n",
    "    )\n",
    "\n",
    "    traders_bronze=(\n",
    "        traders_inc\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(f\"{traders_inc_path}/traders.csv\"))\n",
    "        .withColumn(\"load_type\", lit(\"incremental\"))\n",
    "    )\n",
    "\n",
    "    traders_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"capstone.default.bronze_traders\")\n",
    "    \n",
    "    write_log(\"capstone.default.bronze_traders\", \"default\", \"capstone.default.bronze_log\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"No incremental data found at {traders_inc_path}. Skipping ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5833c57-e4ad-4133-8d1d-069d8d4d72e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Trades data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a84dca5-68d1-471e-9d6e-bf068072c62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trades_inc_path=f\"/Volumes/capstone/default/raw_data/trades/incremental/{ingest_date}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c4dea0-cd8b-4a29-90d7-da4fb52f122e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    trades_inc=(\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(trades_inc_path)\n",
    "    )\n",
    "\n",
    "    trades_bronze=(\n",
    "        trades_inc\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(f\"{trades_inc_path}/trades.csv\"))\n",
    "        .withColumn(\"load_type\", lit(\"incremental\"))\n",
    "    )\n",
    "\n",
    "    trades_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"capstone.default.bronze_trades\")\n",
    "\n",
    "    write_log(\"capstone.default.bronze_trades\", \"default\", \"capstone.default.bronze_log\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"No incremental data found at {trades_inc_path}. Skipping ingestion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aa227c-159b-46a6-a481-fe372fbf553e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fx_rates_inc_path=f\"/Volumes/capstone/default/raw_data/fx_rates/incremental/{ingest_date}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddb65c3-5f25-4ef2-adb1-720b2a47832c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fx_rates_inc=(\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(fx_rates_inc_path)\n",
    "    )\n",
    "\n",
    "    fx_rates_bronze=(\n",
    "        fx_rates_inc\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(f\"{fx_rates_inc_path}/fx_rates.csv\"))\n",
    "        .withColumn(\"load_type\", lit(\"incremental\"))\n",
    "    )\n",
    "\n",
    "    fx_rates_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"capstone.default.bronze_fx_rates\")\n",
    "    \n",
    "    write_log(\"capstone.default.bronze_fx_rates\", \"default\", \"capstone.default.bronze_log\")\n",
    "except AnalysisException as e:\n",
    "    print(f\"No incremental data found at {fx_rates_inc_path}. Skipping ingestion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608cf661-78f4-41de-aa89-768a6310ed36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>log_id</th><th>table_name</th><th>source_system</th><th>ingestion_timestamp</th><th>created_at</th></tr></thead><tbody><tr><td>756f1c5a-c9a7-482f-93d1-bb9dee6ad1b8</td><td>capstone.default.bronze_daily_prices</td><td>default</td><td>2026-01-05T09:36:24.392Z</td><td>2026-01-05T09:36:24.392Z</td></tr><tr><td>0f1748f8-225b-48c3-919b-a434a8880b1b</td><td>capstone.default.bronze_fx_rates</td><td>default</td><td>2026-01-05T09:36:35.544Z</td><td>2026-01-05T09:36:35.544Z</td></tr><tr><td>16eb7e48-bc38-4bd5-8a1a-1224d961c756</td><td>capstone.default.bronze_trades</td><td>default</td><td>2026-01-05T09:36:30.721Z</td><td>2026-01-05T09:36:30.721Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "756f1c5a-c9a7-482f-93d1-bb9dee6ad1b8",
         "capstone.default.bronze_daily_prices",
         "default",
         "2026-01-05T09:36:24.392Z",
         "2026-01-05T09:36:24.392Z"
        ],
        [
         "0f1748f8-225b-48c3-919b-a434a8880b1b",
         "capstone.default.bronze_fx_rates",
         "default",
         "2026-01-05T09:36:35.544Z",
         "2026-01-05T09:36:35.544Z"
        ],
        [
         "16eb7e48-bc38-4bd5-8a1a-1224d961c756",
         "capstone.default.bronze_trades",
         "default",
         "2026-01-05T09:36:30.721Z",
         "2026-01-05T09:36:30.721Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "log_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "table_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "source_system",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "ingestion_timestamp",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "created_at",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 200
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "log_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_system",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingestion_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "created_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM capstone.default.bronze_log;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7153834504835611,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}